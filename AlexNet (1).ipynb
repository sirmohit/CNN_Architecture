{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67tZo1xeU-BH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87305251-f646-4b57-d0bb-69b82a68ca10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tflearn 0.5.0\n",
            "Uninstalling tflearn-0.5.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/tflearn-0.5.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/tflearn/*\n",
            "Proceed (Y/n)? n\n",
            "Collecting git+https://github.com/MihaMarkic/tflearn.git@fix/is_sequence_missing\n",
            "  Cloning https://github.com/MihaMarkic/tflearn.git (to revision fix/is_sequence_missing) to /tmp/pip-req-build-pe7upiul\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MihaMarkic/tflearn.git /tmp/pip-req-build-pe7upiul\n",
            "  Running command git checkout -b fix/is_sequence_missing --track origin/fix/is_sequence_missing\n",
            "  Switched to a new branch 'fix/is_sequence_missing'\n",
            "  Branch 'fix/is_sequence_missing' set up to track remote branch 'fix/is_sequence_missing' from 'origin'.\n",
            "  Resolved https://github.com/MihaMarkic/tflearn.git to commit 6472b8588e758ff4a33a2764d4ee638bbd0e42f0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn==0.5.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn==0.5.0) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn==0.5.0) (9.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tflearn\n",
        "!pip install git+https://github.com/MihaMarkic/tflearn.git@fix/is_sequence_missing\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Conv2D,MaxPooling2D,Activation,Dropout,Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization"
      ],
      "metadata": {
        "id": "7y5sHVhVCuxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GET DATA\n",
        "import tflearn.datasets.oxflower17 as oxflower17\n",
        "from keras.utils import to_categorical\n",
        "x,y =oxflower17.load_data()\n",
        "x_train = x.astype('float32')/255.0\n",
        "y_train = to_categorical(y,num_classes = 17)\n",
        "print(x_train)\n",
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ketGX4OYDv8u",
        "outputId": "f53dc9f7-29b0-4de3-c218-4b92f1b73964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[1.5378702e-05 1.2302962e-04 0.0000000e+00]\n",
            "   [1.5378702e-05 1.2302962e-04 0.0000000e+00]\n",
            "   [1.5378702e-05 1.2302962e-04 0.0000000e+00]\n",
            "   ...\n",
            "   [0.0000000e+00 1.2302962e-04 0.0000000e+00]\n",
            "   [1.5378702e-05 3.6908881e-04 1.6916571e-04]\n",
            "   [1.3840832e-04 7.6893502e-04 5.0749717e-04]]\n",
            "\n",
            "  [[1.5378702e-05 1.2302962e-04 0.0000000e+00]\n",
            "   [1.5378702e-05 1.2302962e-04 0.0000000e+00]\n",
            "   [1.5378702e-05 1.2302962e-04 0.0000000e+00]\n",
            "   ...\n",
            "   [1.5378702e-05 1.3840832e-04 3.0757405e-05]\n",
            "   [1.5378702e-05 3.8446751e-04 1.8454441e-04]\n",
            "   [1.5378701e-04 7.6893502e-04 5.0749717e-04]]\n",
            "\n",
            "  [[1.5378702e-05 1.2302962e-04 0.0000000e+00]\n",
            "   [1.5378702e-05 1.2302962e-04 0.0000000e+00]\n",
            "   [1.5378702e-05 1.2302962e-04 0.0000000e+00]\n",
            "   ...\n",
            "   [1.5378702e-05 1.2302962e-04 1.5378702e-05]\n",
            "   [1.5378702e-05 3.5371011e-04 1.5378701e-04]\n",
            "   [1.5378701e-04 7.2279893e-04 4.6136102e-04]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.5378702e-05 9.2272203e-05 0.0000000e+00]\n",
            "   [1.5378702e-05 9.2272203e-05 0.0000000e+00]\n",
            "   [3.0757405e-05 1.0765091e-04 1.5378702e-05]\n",
            "   ...\n",
            "   [3.0757405e-05 1.0149943e-03 8.3044986e-04]\n",
            "   [3.0757405e-05 6.7666284e-04 5.6901196e-04]\n",
            "   [1.8454441e-04 6.4590544e-04 5.6901196e-04]]\n",
            "\n",
            "  [[1.5378702e-05 9.2272203e-05 0.0000000e+00]\n",
            "   [1.5378702e-05 9.2272203e-05 0.0000000e+00]\n",
            "   [0.0000000e+00 7.6893506e-05 0.0000000e+00]\n",
            "   ...\n",
            "   [6.1514809e-05 1.0918878e-03 7.5355632e-04]\n",
            "   [4.6136101e-05 7.5355632e-04 5.5363326e-04]\n",
            "   [1.3840832e-04 6.4590544e-04 5.0749717e-04]]\n",
            "\n",
            "  [[1.5378702e-05 9.2272203e-05 0.0000000e+00]\n",
            "   [1.5378702e-05 9.2272203e-05 0.0000000e+00]\n",
            "   [0.0000000e+00 7.6893506e-05 0.0000000e+00]\n",
            "   ...\n",
            "   [7.6893506e-05 1.1072665e-03 7.3817762e-04]\n",
            "   [3.0757405e-05 7.6893502e-04 5.2287587e-04]\n",
            "   [1.0765091e-04 6.4590544e-04 4.7673972e-04]]]\n",
            "\n",
            "\n",
            " [[[2.9373318e-03 2.5067283e-03 3.6447521e-03]\n",
            "   [3.0449827e-03 2.6143792e-03 3.6908882e-03]\n",
            "   [3.3064208e-03 2.9219531e-03 3.8292964e-03]\n",
            "   ...\n",
            "   [1.4763552e-03 2.0607461e-03 8.7658595e-04]\n",
            "   [1.2302961e-03 1.7685506e-03 5.9976935e-04]\n",
            "   [1.1380239e-03 1.6608997e-03 4.9211847e-04]]\n",
            "\n",
            "  [[2.7066513e-03 2.1376396e-03 3.7677817e-03]\n",
            "   [2.6605153e-03 2.1222609e-03 3.6908882e-03]\n",
            "   [2.9373318e-03 2.4298348e-03 3.7985391e-03]\n",
            "   ...\n",
            "   [1.2610535e-03 1.8300654e-03 6.7666284e-04]\n",
            "   [1.0765091e-03 1.6147635e-03 4.6136102e-04]\n",
            "   [8.3044986e-04 1.3533257e-03 1.8454441e-04]]\n",
            "\n",
            "  [[2.9373318e-03 2.3221839e-03 3.8139177e-03]\n",
            "   [2.5836218e-03 1.9838526e-03 3.5371012e-03]\n",
            "   [2.5374857e-03 1.9530950e-03 3.5217225e-03]\n",
            "   ...\n",
            "   [1.1380239e-03 1.6608997e-03 5.5363326e-04]\n",
            "   [9.5347944e-04 1.4763552e-03 3.5371011e-04]\n",
            "   [7.8431371e-04 1.3225683e-03 1.8454441e-04]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.5378702e-05 1.5378702e-05 1.5378702e-05]\n",
            "   [1.5378702e-05 1.5378702e-05 1.5378702e-05]\n",
            "   [1.5378702e-05 1.5378702e-05 1.5378702e-05]\n",
            "   ...\n",
            "   [2.3990774e-03 1.9530950e-03 1.5993848e-03]\n",
            "   [2.2606691e-03 1.8300654e-03 1.4917339e-03]\n",
            "   [2.0299887e-03 1.6147635e-03 1.2918109e-03]]\n",
            "\n",
            "  [[1.5378702e-05 1.5378702e-05 1.5378702e-05]\n",
            "   [1.5378702e-05 1.5378702e-05 1.5378702e-05]\n",
            "   [1.5378702e-05 1.5378702e-05 1.5378702e-05]\n",
            "   ...\n",
            "   [2.4452135e-03 1.9223376e-03 1.5378700e-03]\n",
            "   [2.3068052e-03 1.8146867e-03 1.4763552e-03]\n",
            "   [2.1376396e-03 1.6762784e-03 1.3533257e-03]]\n",
            "\n",
            "  [[1.5378702e-05 1.5378702e-05 1.5378702e-05]\n",
            "   [1.5378702e-05 1.5378702e-05 1.5378702e-05]\n",
            "   [1.5378702e-05 1.5378702e-05 1.5378702e-05]\n",
            "   ...\n",
            "   [2.3529413e-03 1.7839293e-03 1.3687044e-03]\n",
            "   [2.1068822e-03 1.5993848e-03 1.2302961e-03]\n",
            "   [1.9377163e-03 1.4763552e-03 1.1226452e-03]]]\n",
            "\n",
            "\n",
            " [[[4.6136102e-04 1.2610535e-03 1.9838526e-03]\n",
            "   [4.6136102e-04 1.2610535e-03 1.9838526e-03]\n",
            "   [4.6136102e-04 1.2610535e-03 1.9838526e-03]\n",
            "   ...\n",
            "   [3.0757405e-05 7.8431371e-04 1.5224913e-03]\n",
            "   [1.5378702e-05 7.6893502e-04 1.4917339e-03]\n",
            "   [0.0000000e+00 7.6893502e-04 1.4917339e-03]]\n",
            "\n",
            "  [[4.7673972e-04 1.2764322e-03 1.9992313e-03]\n",
            "   [4.7673972e-04 1.2764322e-03 1.9992313e-03]\n",
            "   [4.7673972e-04 1.2764322e-03 1.9992313e-03]\n",
            "   ...\n",
            "   [3.0757405e-05 8.1507111e-04 1.5378700e-03]\n",
            "   [0.0000000e+00 7.8431371e-04 1.5071126e-03]\n",
            "   [0.0000000e+00 7.8431371e-04 1.5071126e-03]]\n",
            "\n",
            "  [[4.9211847e-04 1.2918109e-03 2.0146100e-03]\n",
            "   [4.9211847e-04 1.2918109e-03 2.0146100e-03]\n",
            "   [4.9211847e-04 1.2918109e-03 2.0146100e-03]\n",
            "   ...\n",
            "   [3.0757405e-05 8.3044986e-04 1.5532487e-03]\n",
            "   [0.0000000e+00 7.9969241e-04 1.5224913e-03]\n",
            "   [0.0000000e+00 7.9969241e-04 1.5224913e-03]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.0453674e-03 1.8300654e-03 5.8439065e-04]\n",
            "   [1.2764322e-03 1.0918878e-03 4.3060363e-04]\n",
            "   [1.2302962e-04 3.0757405e-05 0.0000000e+00]\n",
            "   ...\n",
            "   [1.1687813e-03 1.0457517e-03 8.1507111e-04]\n",
            "   [1.1995387e-03 1.0303730e-03 8.1507111e-04]\n",
            "   [1.2149174e-03 1.0303730e-03 8.1507111e-04]]\n",
            "\n",
            "  [[2.1068822e-03 1.8915802e-03 6.4590544e-04]\n",
            "   [1.1226452e-03 9.5347944e-04 3.2295272e-04]\n",
            "   [9.2272203e-05 1.5378702e-05 0.0000000e+00]\n",
            "   ...\n",
            "   [1.1995387e-03 1.0765091e-03 7.5355632e-04]\n",
            "   [1.2302961e-03 1.0611304e-03 7.5355632e-04]\n",
            "   [1.2456748e-03 1.0611304e-03 7.5355632e-04]]\n",
            "\n",
            "  [[2.1376396e-03 1.9223376e-03 6.7666284e-04]\n",
            "   [1.0149943e-03 8.4582856e-04 2.3068051e-04]\n",
            "   [3.0757405e-05 0.0000000e+00 0.0000000e+00]\n",
            "   ...\n",
            "   [1.2764322e-03 1.1380239e-03 7.5355632e-04]\n",
            "   [1.2764322e-03 1.0918878e-03 7.3817762e-04]\n",
            "   [1.2764322e-03 1.0918878e-03 7.3817762e-04]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[2.3068052e-03 2.0607461e-03 1.8762015e-03]\n",
            "   [2.0915035e-03 1.8608228e-03 1.6916571e-03]\n",
            "   [1.9530950e-03 1.7377932e-03 1.5840061e-03]\n",
            "   ...\n",
            "   [8.4582856e-04 1.4763552e-03 4.4598232e-04]\n",
            "   [9.3810074e-04 1.4763552e-03 4.3060363e-04]\n",
            "   [1.0149943e-03 1.5071126e-03 4.4598232e-04]]\n",
            "\n",
            "  [[2.2299117e-03 2.0146100e-03 1.8454441e-03]\n",
            "   [2.0915035e-03 1.8915802e-03 1.7377932e-03]\n",
            "   [1.9377163e-03 1.7685506e-03 1.6301422e-03]\n",
            "   ...\n",
            "   [8.4582856e-04 1.4609765e-03 4.6136102e-04]\n",
            "   [9.3810074e-04 1.4763552e-03 4.4598232e-04]\n",
            "   [1.0303730e-03 1.5071126e-03 4.7673972e-04]]\n",
            "\n",
            "  [[2.0146100e-03 1.8608228e-03 1.7377932e-03]\n",
            "   [1.9377163e-03 1.8300654e-03 1.6916571e-03]\n",
            "   [1.8146867e-03 1.6916571e-03 1.5686274e-03]\n",
            "   ...\n",
            "   [7.9969241e-04 1.4148405e-03 4.6136102e-04]\n",
            "   [8.4582856e-04 1.3840831e-03 4.1522493e-04]\n",
            "   [8.9196465e-04 1.4148405e-03 4.3060363e-04]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[5.3825456e-04 5.3825456e-04 5.6901196e-04]\n",
            "   [6.1514805e-04 6.1514805e-04 6.6128414e-04]\n",
            "   [6.6128414e-04 6.4590544e-04 7.2279893e-04]\n",
            "   ...\n",
            "   [4.9211847e-04 5.3825456e-04 7.0742023e-04]\n",
            "   [5.3825456e-04 5.5363326e-04 6.4590544e-04]\n",
            "   [4.9211847e-04 5.0749717e-04 5.6901196e-04]]\n",
            "\n",
            "  [[6.3052675e-04 6.3052675e-04 6.7666284e-04]\n",
            "   [4.7673972e-04 4.7673972e-04 5.0749717e-04]\n",
            "   [2.9219533e-04 2.7681663e-04 3.5371011e-04]\n",
            "   ...\n",
            "   [4.6136102e-04 5.0749717e-04 6.6128414e-04]\n",
            "   [6.3052675e-04 6.6128414e-04 7.3817762e-04]\n",
            "   [5.8439065e-04 5.9976935e-04 6.6128414e-04]]\n",
            "\n",
            "  [[1.9992310e-04 2.1530181e-04 2.7681663e-04]\n",
            "   [1.8454441e-04 1.9992310e-04 2.6143793e-04]\n",
            "   [2.1530181e-04 2.3068051e-04 2.9219533e-04]\n",
            "   ...\n",
            "   [4.4598232e-04 4.9211847e-04 5.6901196e-04]\n",
            "   [5.5363326e-04 5.8439065e-04 6.6128414e-04]\n",
            "   [4.9211847e-04 5.0749717e-04 5.8439065e-04]]]\n",
            "\n",
            "\n",
            " [[[8.4582856e-04 1.5993848e-03 2.6605153e-03]\n",
            "   [8.4582856e-04 1.5993848e-03 2.6605153e-03]\n",
            "   [8.4582856e-04 1.5993848e-03 2.6605153e-03]\n",
            "   ...\n",
            "   [7.9969241e-04 1.7070358e-03 2.7220300e-03]\n",
            "   [8.1507111e-04 1.7224145e-03 2.7374087e-03]\n",
            "   [8.1507111e-04 1.7224145e-03 2.7374087e-03]]\n",
            "\n",
            "  [[8.4582856e-04 1.5993848e-03 2.6605153e-03]\n",
            "   [8.4582856e-04 1.5993848e-03 2.6605153e-03]\n",
            "   [8.4582856e-04 1.5993848e-03 2.6605153e-03]\n",
            "   ...\n",
            "   [8.1507111e-04 1.7224145e-03 2.7374087e-03]\n",
            "   [8.1507111e-04 1.7224145e-03 2.7374087e-03]\n",
            "   [8.1507111e-04 1.7224145e-03 2.7374087e-03]]\n",
            "\n",
            "  [[8.4582856e-04 1.5993848e-03 2.6605153e-03]\n",
            "   [8.4582856e-04 1.5993848e-03 2.6605153e-03]\n",
            "   [8.4582856e-04 1.5993848e-03 2.6605153e-03]\n",
            "   ...\n",
            "   [8.3044986e-04 1.7377932e-03 2.7527874e-03]\n",
            "   [8.1507111e-04 1.7224145e-03 2.7374087e-03]\n",
            "   [8.1507111e-04 1.7224145e-03 2.7374087e-03]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.9680892e-03 3.0296040e-03 2.3068052e-03]\n",
            "   [2.6605153e-03 2.8143022e-03 2.0915035e-03]\n",
            "   [2.3529413e-03 2.6451366e-03 1.9223376e-03]\n",
            "   ...\n",
            "   [1.9223376e-03 2.6758939e-03 3.1833909e-03]\n",
            "   [1.9377163e-03 2.6912726e-03 3.1987696e-03]\n",
            "   [1.9377163e-03 2.6912726e-03 3.1987696e-03]]\n",
            "\n",
            "  [[3.2141483e-03 3.0603614e-03 1.2610535e-03]\n",
            "   [2.9988466e-03 2.9680892e-03 1.3994618e-03]\n",
            "   [2.6912726e-03 2.8450596e-03 1.6455210e-03]\n",
            "   ...\n",
            "   [1.9377163e-03 2.6912726e-03 3.1987696e-03]\n",
            "   [1.9377163e-03 2.6912726e-03 3.1987696e-03]\n",
            "   [1.9377163e-03 2.6912726e-03 3.1987696e-03]]\n",
            "\n",
            "  [[3.5986160e-03 3.3833142e-03 7.8431371e-04]\n",
            "   [3.3986929e-03 3.2910421e-03 1.1226452e-03]\n",
            "   [3.0911188e-03 3.1526336e-03 1.7685506e-03]\n",
            "   ...\n",
            "   [1.9377163e-03 2.6912726e-03 3.1987696e-03]\n",
            "   [1.9377163e-03 2.6912726e-03 3.1987696e-03]\n",
            "   [1.9377163e-03 2.6912726e-03 3.1987696e-03]]]\n",
            "\n",
            "\n",
            " [[[3.0757402e-04 6.4590544e-04 6.1514809e-05]\n",
            "   [3.5371011e-04 6.9204153e-04 1.0765091e-04]\n",
            "   [3.5371011e-04 6.9204153e-04 1.0765091e-04]\n",
            "   ...\n",
            "   [2.3068051e-04 9.2272203e-05 0.0000000e+00]\n",
            "   [2.4605924e-04 1.0765091e-04 0.0000000e+00]\n",
            "   [2.4605924e-04 1.0765091e-04 0.0000000e+00]]\n",
            "\n",
            "  [[3.0757402e-04 6.4590544e-04 4.6136101e-05]\n",
            "   [3.3833142e-04 6.7666284e-04 7.6893506e-05]\n",
            "   [3.6908881e-04 7.0742023e-04 1.0765091e-04]\n",
            "   ...\n",
            "   [2.3068051e-04 9.2272203e-05 0.0000000e+00]\n",
            "   [2.3068051e-04 1.0765091e-04 0.0000000e+00]\n",
            "   [2.3068051e-04 1.0765091e-04 0.0000000e+00]]\n",
            "\n",
            "  [[2.9219533e-04 6.3052675e-04 1.5378702e-05]\n",
            "   [3.2295272e-04 6.6128414e-04 6.1514809e-05]\n",
            "   [3.6908881e-04 7.0742023e-04 9.2272203e-05]\n",
            "   ...\n",
            "   [2.1530181e-04 1.0765091e-04 0.0000000e+00]\n",
            "   [1.9992310e-04 9.2272203e-05 0.0000000e+00]\n",
            "   [1.9992310e-04 9.2272203e-05 0.0000000e+00]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[4.6136101e-05 3.0757405e-05 3.0757405e-05]\n",
            "   [3.0757405e-05 3.0757405e-05 4.6136101e-05]\n",
            "   [1.5378702e-05 3.0757405e-05 7.6893506e-05]\n",
            "   ...\n",
            "   [1.5532487e-03 9.6885813e-04 4.3060363e-04]\n",
            "   [1.5686274e-03 9.8423695e-04 4.3060363e-04]\n",
            "   [1.4763552e-03 8.9196465e-04 3.3833142e-04]]\n",
            "\n",
            "  [[1.5378702e-05 1.5378702e-05 0.0000000e+00]\n",
            "   [1.5378702e-05 1.5378702e-05 0.0000000e+00]\n",
            "   [3.0757405e-05 3.0757405e-05 4.6136101e-05]\n",
            "   ...\n",
            "   [1.5532487e-03 1.0303730e-03 4.7673972e-04]\n",
            "   [1.6147635e-03 1.0765091e-03 5.0749717e-04]\n",
            "   [1.4763552e-03 9.3810074e-04 3.5371011e-04]]\n",
            "\n",
            "  [[1.5378702e-05 4.6136101e-05 0.0000000e+00]\n",
            "   [1.5378702e-05 1.5378702e-05 0.0000000e+00]\n",
            "   [3.0757405e-05 3.0757405e-05 3.0757405e-05]\n",
            "   ...\n",
            "   [1.5686274e-03 1.0457517e-03 5.0749717e-04]\n",
            "   [1.6608997e-03 1.1380239e-03 5.5363326e-04]\n",
            "   [1.5071126e-03 9.8423695e-04 3.9984621e-04]]]]\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Convolutional Layer\n",
        "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation before passing it to the next layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Dense Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(17))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIlSKmsoH8P2",
        "outputId": "5845699d-9f19-4169-ea01-8dbf9a017478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_16 (Conv2D)          (None, 54, 54, 96)        34944     \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 54, 54, 96)        0         \n",
            "                                                                 \n",
            " max_pooling2d_11 (MaxPooli  (None, 26, 26, 96)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_13 (Ba  (None, 26, 26, 96)        384       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 26, 26, 256)       614656    \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 26, 26, 256)       0         \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPooli  (None, 12, 12, 256)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_14 (Ba  (None, 12, 12, 256)       1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 10, 10, 384)       885120    \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 10, 10, 384)       0         \n",
            "                                                                 \n",
            " batch_normalization_15 (Ba  (None, 10, 10, 384)       1536      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 8, 8, 384)         1327488   \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 8, 8, 384)         0         \n",
            "                                                                 \n",
            " batch_normalization_16 (Ba  (None, 8, 8, 384)         1536      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 6, 6, 256)         884992    \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPooli  (None, 2, 2, 256)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_17 (Ba  (None, 2, 2, 256)         1024      \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4096)              4198400   \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_18 (Ba  (None, 4096)              16384     \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_19 (Ba  (None, 4096)              16384     \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 17)                69649     \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 17)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24834833 (94.74 MB)\n",
            "Trainable params: 24815697 (94.66 MB)\n",
            "Non-trainable params: 19136 (74.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "qhPMwKQoIhi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "model.fit(x_train,y_train,batch_size = 64,epochs = 5,verbose = 1,validation_split = 0.2,shuffle = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZKJjghIO_FY",
        "outputId": "5169ab88-e16b-4dbe-c01f-267cb3c3b841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 1088 samples, validate on 272 samples\n",
            "Epoch 1/5\n",
            "1088/1088 [==============================] - 3s 2ms/sample - loss: 3.8682 - acc: 0.2279 - val_loss: 3.1977 - val_acc: 0.0662\n",
            "Epoch 2/5\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 2.4865 - acc: 0.3805 - val_loss: 6.4365 - val_acc: 0.0662\n",
            "Epoch 3/5\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 1.9458 - acc: 0.4669 - val_loss: 5.9385 - val_acc: 0.0772\n",
            "Epoch 4/5\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 1.6479 - acc: 0.5156 - val_loss: 6.6623 - val_acc: 0.0662\n",
            "Epoch 5/5\n",
            "1088/1088 [==============================] - 1s 1ms/sample - loss: 1.2987 - acc: 0.5974 - val_loss: 8.0663 - val_acc: 0.0625\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f38012e7910>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ShXpON20PgTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}